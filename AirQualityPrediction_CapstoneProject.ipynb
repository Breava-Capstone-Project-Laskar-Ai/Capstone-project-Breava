{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8tl-HXHkdR8",
        "outputId": "25133ffc-3859-4f68-a91b-19a6771a5994"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.11.0 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.11.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG7RnjHwbS5d",
        "outputId": "8a1e05c7-0212-4bc4-9663-738312dbe557"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires keras>=3.5.0, but you have keras 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oHctvaxZMqqz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "a3d32c90-6ab1-4313-cb57-996fe96339f5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'inspect' has no attribute 'ArgSpec'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1ffc7979a23d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeseriesGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/api/_v2/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayout_map\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_coordinator_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/engine/keras_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Public Keras utilities.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/saving/legacy/serialization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_contextlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/utils/tf_inspect.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mArgSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'inspect' has no attribute 'ArgSpec'"
          ]
        }
      ],
      "source": [
        "# Import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "adhang_air_quality_in_yogyakarta_indonesia_2021_path = kagglehub.dataset_download('adhang/air-quality-in-yogyakarta-indonesia-2021')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "wsD0Y-0pPyNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Tampilkan path hasil unduhan\n",
        "print(adhang_air_quality_in_yogyakarta_indonesia_2021_path)\n",
        "\n",
        "# Lihat file/folder di dalamnya\n",
        "print(os.listdir(adhang_air_quality_in_yogyakarta_indonesia_2021_path))"
      ],
      "metadata": {
        "id": "y4-XFNezQv8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membaca dataset januari dari file CSV\n",
        "df1 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-jan-2021.csv\")\n",
        "\n",
        "# Membaca dataset februari dari file CSV\n",
        "df2 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-feb-2021.csv\")\n",
        "\n",
        "# Membaca dataset maret dari file CSV\n",
        "df3 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-mar-2021.csv\")\n",
        "\n",
        "# Membaca dataset april dari file CSV\n",
        "df4 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-apr-2021.csv\")\n",
        "\n",
        "# Membaca dataset mei dari file CSV\n",
        "df5 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-may-2021.csv\")\n",
        "\n",
        "# Membaca dataset juni dari file CSV\n",
        "df6 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-jun-2021.csv\")\n",
        "\n",
        "# Membaca dataset juli dari file CSV\n",
        "df7 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-jul-2021.csv\")\n",
        "\n",
        "# Membaca dataset agustus dari file CSV\n",
        "df8 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-aug-2021.csv\")\n",
        "\n",
        "# Membaca dataset september dari file CSV\n",
        "df9 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-sep-2021.csv\")\n",
        "\n",
        "# Membaca dataset  oktober dari file CSV\n",
        "df10 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-oct-2021.csv\")\n",
        "\n",
        "# Membaca dataset  november dari file CSV\n",
        "df11 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-nov-2021.csv\")\n",
        "\n",
        "# Membaca dataset  desember dari file CSV\n",
        "df12 = pd.read_csv(\"/root/.cache/kagglehub/datasets/adhang/air-quality-in-yogyakarta-indonesia-2021/versions/1/psi-jogja-dec-2021.csv\")\n",
        "\n",
        "# Menyatukan kedua dataset ke dalam satu dataframe\n",
        "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12])\n",
        "\n",
        "# Menampilkan hasil penggabungan\n",
        "print(df)"
      ],
      "metadata": {
        "id": "CEDjY-9GM0M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "zPi02GJiROrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "fn3QNwqjRXKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Gabungkan kolom Date dan Time\n",
        "df['datetime'] = df['Date'] + ' ' + df['Time']\n",
        "\n",
        "# Konversi ke datetime format\n",
        "df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "# Set datetime sebagai index\n",
        "df = df.set_index('datetime')\n",
        "\n",
        "# Drop kolom lama jika tidak dibutuhkan\n",
        "df = df.drop(['Date', 'Time'], axis=1)\n",
        "\n",
        "# Ekstrak waktu\n",
        "df['year'] = df.index.year\n",
        "df['month'] = df.index.month\n",
        "df['day'] = df.index.dayofweek  # 0 = Monday, ..., 6 = Sunday\n",
        "df['hour'] = df.index.hour\n",
        "\n",
        "# Lihat hasil\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PsarZtCiP_9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "dvhKjp0nQoxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(x='month',y='PM2.5',data=df)\n",
        "plt.xlabel('month')\n",
        "plt.ylabel('aqi')"
      ],
      "metadata": {
        "id": "NwFRcbs3RMP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "sns.set(rc = {'figure.figsize':(15,8)})\n",
        "ax = sns.boxplot(x=\"month\", y=\"PM2.5\", data=df)"
      ],
      "metadata": {
        "id": "iAPi-LX6Rugu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(x='day',y='PM2.5',data=df)\n",
        "plt.xlabel('day')\n",
        "plt.ylabel('PM2.5')"
      ],
      "metadata": {
        "id": "7EqwB6gER2LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "sns.set(rc = {'figure.figsize':(15,8)})\n",
        "ax = sns.boxplot(x=\"day\", y=\"PM2.5\", data=df)"
      ],
      "metadata": {
        "id": "yRg8NYX9SAYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UTC Time\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(x='hour',y='PM2.5',data=df)\n",
        "plt.xlabel('hour')\n",
        "plt.ylabel('PM2.5')"
      ],
      "metadata": {
        "id": "oEYy2_oMSG0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "sns.set(rc = {'figure.figsize':(15,8)})\n",
        "ax = sns.boxplot(x=\"hour\", y=\"PM2.5\", data=df)"
      ],
      "metadata": {
        "id": "jxaGqkF3SPmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Ambil hanya kolom numerik\n",
        "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Plot korelasi antar kolom numerik\n",
        "sns.heatmap(numeric_df.corr(), cbar=True, annot=True, cmap='Blues')\n",
        "\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bhQi7cozSYrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_input = df.drop(['Category','Critical Component', 'Max', 'year', 'month', 'hour', 'day'], axis=1)"
      ],
      "metadata": {
        "id": "WhzCBLS0SqyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_input.info()"
      ],
      "metadata": {
        "id": "olR7ydPwTDqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_input"
      ],
      "metadata": {
        "id": "Zub1nIypTLJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot null data\n",
        "sns.heatmap(df_input.isnull(),cbar=False,cmap='viridis')"
      ],
      "metadata": {
        "id": "3YnEpITtTmN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total null data\n",
        "print(df_input.isnull().sum())\n",
        "print('shape:', df_input.shape)"
      ],
      "metadata": {
        "id": "bqZRTKKETqiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop null value\n",
        "df_input.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "imuhBhreTt_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total null data\n",
        "print(df_input.isnull().sum())\n",
        "print('shape:', df_input.shape)"
      ],
      "metadata": {
        "id": "zXQyzubrTv2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_input.describe()"
      ],
      "metadata": {
        "id": "dnYl9FM-TyTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequences)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps_in\n",
        "\t\tout_end_ix = end_ix + n_steps_out\n",
        "\t\t# check if we are beyond the dataset\n",
        "\t\tif out_end_ix > len(sequences):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y)"
      ],
      "metadata": {
        "id": "xepKguOJT2Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset to train and test\n",
        "total_dataset = len(df_input)\n",
        "df_train = df_input[:int(total_dataset*0.76)]\n",
        "df_test = df_input[int(total_dataset*0.76):total_dataset]\n",
        "\n",
        "# scaled\n",
        "scaler = MinMaxScaler()\n",
        "df_train_scaled = scaler.fit_transform(df_train)\n",
        "df_test_scaled = scaler.fit_transform(df_test)\n",
        "\n",
        "print('Data for train:', df_train.shape)\n",
        "print('Total day for train:', df_train.shape[0]/24)\n",
        "print('\\nData for test:', df_test.shape)\n",
        "print('Total day for test:', df_test.shape[0]/24)"
      ],
      "metadata": {
        "id": "v548VpjAT4O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a number of time steps\n",
        "# n_steps_in history data for predict n_steps_out forecast data\n",
        "n_steps_in, n_steps_out = 6, 3\n",
        "\n",
        "# Convert into input/output\n",
        "x_train, y_train = split_sequences(df_train_scaled, n_steps_in, n_steps_out)\n",
        "x_test, y_test = split_sequences(df_test_scaled, n_steps_in, n_steps_out)\n",
        "\n",
        "# The dataset knows the number of features, e.g. 2\n",
        "n_features = x_train.shape[2]"
      ],
      "metadata": {
        "id": "GvK-DrlNT64I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the x_train structure\n",
        "print('Data input structure for training')\n",
        "print(x_train[0])\n",
        "\n",
        "print('\\nData output structure for validation')\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "id": "mRPJbvNtT9jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total features:', n_features)\n",
        "print('Total train data:', x_train.shape)\n",
        "print('Total validation train data:', y_train.shape)\n",
        "print('Total test data:', x_test.shape)\n",
        "print('Total validation test data:', y_test.shape)"
      ],
      "metadata": {
        "id": "n3GXvFd3T_xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))\n",
        "model.add(RepeatVector(n_steps_out))\n",
        "model.add(LSTM(200, activation='relu', return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(n_features)))"
      ],
      "metadata": {
        "id": "6AEk8Z4HUB6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "57bO0j-PUEv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "epoch = 50\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse',\n",
        "              metrics=[tf.metrics.MeanAbsoluteError()])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=epoch, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "wNxYKDqsUHtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test)\n",
        "print(predictions.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "DCNwiRQ_VGZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape according to predictions shape\n",
        "predictions = np.reshape(predictions, (1372*3, 6))\n",
        "real = np.reshape(y_test, (1372*3, 6))\n",
        "\n",
        "# rescale predictions\n",
        "predictions = pd.DataFrame(predictions)\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "predictions = pd.DataFrame(predictions)\n",
        "\n",
        "real = pd.DataFrame(real)\n",
        "real = scaler.inverse_transform(real)\n",
        "real = pd.DataFrame(real)\n",
        "\n",
        "print(predictions.shape)\n",
        "print(real.shape)"
      ],
      "metadata": {
        "id": "BCNzqj3NVW3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "eaxaAOR-WHg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real"
      ],
      "metadata": {
        "id": "zpsF7WVrWJtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.DataFrame()"
      ],
      "metadata": {
        "id": "4wvr3TYBWNyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final['PM10'] = real[0]\n",
        "df_final['PM2.5'] = real[1]\n",
        "df_final['SO2'] = real[2]\n",
        "df_final['CO'] = real[3]\n",
        "df_final['O3'] = real[4]\n",
        "df_final['NO2'] = real[5]\n",
        "\n",
        "df_final['PM10_pred'] = predictions[0]\n",
        "df_final['PM2.5_pred'] = predictions[1]\n",
        "df_final['SO2_pred'] = predictions[2]\n",
        "df_final['CO_pred'] = predictions[3]\n",
        "df_final['O3_pred'] = predictions[4]\n",
        "df_final['NO2_pred'] = predictions[5]"
      ],
      "metadata": {
        "id": "VWTt_46CWP-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final"
      ],
      "metadata": {
        "id": "BkWRzqMKW38z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mpl.rcParams['figure.figsize'] = (20,10)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "df_final[['PM10', 'PM10_pred']].plot()\n",
        "df_final[['PM2.5', 'PM2.5_pred']].plot()\n",
        "df_final[['SO2', 'SO2_pred']].plot()\n",
        "df_final[['CO', 'CO_pred']].plot()\n",
        "df_final[['O3', 'O3_pred']].plot()\n",
        "df_final[['NO2', 'NO2_pred']].plot()"
      ],
      "metadata": {
        "id": "DYhg3nfeW9aT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_final.columns)"
      ],
      "metadata": {
        "id": "FBGjlCjjYTVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "indicator = ['PM10', 'PM2.5', 'SO2', 'O3', 'NO2']\n",
        "for i in indicator:\n",
        "    mse = mean_squared_error(df_final[f'{i}'], df_final[f'{i}_pred'])\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(df_final[f'{i}'], df_final[f'{i}_pred'])\n",
        "\n",
        "    print(f'{i} - RMSE: {rmse}, MSE: {mse}, MAE: {mae}')"
      ],
      "metadata": {
        "id": "L1W1RCGQaK-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model\n",
        "model.save('air_quality_lstm_model.h5')\n",
        "print(\"Model berhasil disimpan dalam format H5.\")"
      ],
      "metadata": {
        "id": "lU5rFOKbSRGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_csv('hasil_prediksi_air_quality.csv', index=False)"
      ],
      "metadata": {
        "id": "SoM4YeAbV5lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Simpan scaler yang digunakan untuk train\n",
        "joblib.dump(scaler, \"scaler_air_quality.pkl\")"
      ],
      "metadata": {
        "id": "ovwJCOJJ0GlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan MinMaxScaler hanya untuk kolom PM2.5\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Ambil hanya kolom PM2.5 untuk training\n",
        "pm25_train = df_train[['PM2.5']]\n",
        "\n",
        "# Fit scaler\n",
        "scaler_pm25 = MinMaxScaler()\n",
        "scaler_pm25.fit(pm25_train)\n",
        "\n",
        "# Simpan parameter min_ dan scale_ ke file numpy\n",
        "np.save(\"scaler_pm25.npy\", [scaler_pm25.min_, scaler_pm25.scale_])"
      ],
      "metadata": {
        "id": "eMzJAJSa2Shr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "UPJ6ccxn5mln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "metadata": {
        "id": "dwQmF5X_BibE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}